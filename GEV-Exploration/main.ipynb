{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Python & R Integration with .NET Interactive\n",
    "\n",
    "In this section, I demonstrate how to connect Python and R kernels to the .NET Interactive system, allowing for seamless use of Python, R, and other languages together in a single notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The `#!connect jupyter` feature is in preview. Please report any feedback or issues at https://github.com/dotnet/interactive/issues/new/choose."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Kernel added: #!phd-python"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!connect jupyter --kernel-name phd-python --conda-env phd --kernel-spec python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The `#!connect jupyter` feature is in preview. Please report any feedback or issues at https://github.com/dotnet/interactive/issues/new/choose."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Kernel added: #!Rkernel"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!connect jupyter --kernel-name Rkernel --conda-env phd --kernel-spec ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🐍 Python: Library Import and Global Variable Definition\n",
    "\n",
    "In this section, I import my necessary libraries 📚 and define global variables 🌐 (mostly path-related). These variables can be reused throughout the code to adapt to different locations or users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Third-party library imports\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as ftr\n",
    "import cftime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "import numdifftools as nd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "from scipy.stats import chi2, norm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import xarray as xr\n",
    "\n",
    "# IPython display tools\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "#Path to change !!!\n",
    "DATA_PATH_PY = r\"c:\\Users\\bobel\\OneDrive - Université de Namur\\Data\"\n",
    "DATA_PATH_R = \"c:\\\\Users\\\\bobel\\\\OneDrive - Université de Namur\\\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEV class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "from EVT_Classes import * \n",
    "\n",
    "EOBS = pd.read_csv(r\"c:\\ThesisData\\Blockmax\\EOBS_blockmax.csv\")\n",
    "\n",
    "exog = {'scale':EOBS[\"tempanomalyMean\"].values,'location':EOBS['tempanomalyMean'].values}\n",
    "#initi_params = np.array([52, 0.1,12,0.1,0.07])\n",
    "# Initialize and use EVTModel\n",
    "model1 = GEVTradowsky(endog=EOBS[\"prmax\"].values,exog=exog,full_output=True)\n",
    "a = model1.fit()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Function Corpus\n",
    "\n",
    "In this section, I define my python functions for later use. All functions are well-documented with descriptions, and examples are provided for each function to illustrate how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Section 1: Masking Functions\n",
    "\n",
    "This section contains specialized functions for cropping space-time datacubes, either in time, space, or both. These functions are essential for handling smaller datasets and making data manipulation more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def mask_square(ds, center, side_length):\n",
    "    \"\"\"\n",
    "    Masks the dataset to include only points within a specified square.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The xarray dataset containing latitude and longitude data.\n",
    "    center (tuple): A tuple of coordinates (longitude, latitude) representing the center of the square.\n",
    "    side_length (float): The side length of the square in degrees.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A new xarray dataset containing only the points within the specified square.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = ds.copy()\n",
    "    # Calculate the half side length\n",
    "    half_side = side_length / 2\n",
    "\n",
    "    # Define the boundaries of the square\n",
    "    min_lon = center[0] - side_length\n",
    "    max_lon = center[0] + side_length\n",
    "    min_lat = center[1] - side_length\n",
    "    max_lat = center[1] + side_length\n",
    "\n",
    "    # Apply the mask to select points within the square\n",
    "    masked_ds = ds.where((ds['lon']>min_lon )&(ds['lon']< max_lon)&(ds['lat']<max_lat)&(ds['lat']>min_lat),drop = True)\n",
    "    return masked_ds\n",
    "\n",
    "def mask_time(ds, season='summer'):\n",
    "    \"\"\"\n",
    "    Mask the dataset to include only specific seasons, such as summer (April to September) or winter (October to March).\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The xarray dataset containing a time dimension with cftime objects.\n",
    "    season (str, optional): The season to mask. Supports 'summer' (April to September) or 'winter' (October to March). Default is 'summer'.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A new xarray dataset containing only the specified season.\n",
    "\n",
    "    Example usage:\n",
    "    ds_summer = mask_time(datacube, season='summer')\n",
    "    ds_winter = mask_time(datacube, season='winter')\n",
    "    \"\"\"\n",
    "    if season == 'summer':\n",
    "        ds = ds.sel(time=[t for t in ds['time'].values if t.month in [4, 5, 6, 7, 8, 9]])\n",
    "    elif season == 'winter':\n",
    "        ds = ds.sel(time=[t for t in ds['time'].values if t.month in [10, 11, 12, 1, 2, 3]])\n",
    "    else:\n",
    "        raise ValueError(\"Supported seasons are 'summer' and 'winter'.\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📂 Loading Section: DataCube Loading Functions\n",
    "\n",
    "This section is specialized in loading space-time datacubes from various file formats, such as NetCDF, while applying the masks built in **Section 1**. These masks help reduce the dataset size by cropping the datacube in time, space, or both, making it more efficient to work with and easier to manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def convert_360_to_180(values):\n",
    "        return xr.where(values < 180, values, values - 360)\n",
    "\n",
    "def load_netcdf(filepath, coords=('pr','lat','lon'), use_cftime=True):\n",
    "    \"\"\"\n",
    "    Load a NETCDF file and return it as an xarray dataset.\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): Path to the NETCDF file to be loaded.\n",
    "    use_cftime (bool): Whether to use cftime for handling dates out of numpy range. Default is True.\n",
    "    Returns:\n",
    "    xarray.Dataset: The loaded dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = xr.open_dataset(filepath, engine='netcdf4', use_cftime=use_cftime)\n",
    "        coord_names = list(dataset.coords.keys())\n",
    "        datavars_names = list(dataset.data_vars.keys())\n",
    "        if 'pr' not in datavars_names:\n",
    "            dataset = dataset.rename({coords[0]: 'pr'})\n",
    "            dataset['pr'].attrs['units'] = 'mm/day'\n",
    "        else:\n",
    "            dataset['pr'] = dataset['pr'] * 86400\n",
    "            dataset['pr'].attrs['units'] = 'mm/day'\n",
    "        if 'lat' not in coord_names:\n",
    "            dataset = dataset.rename({coords[1]: 'lat'})\n",
    "        if 'lon' not in coord_names:\n",
    "            dataset = dataset.rename({coords[2]: 'lon'})\n",
    "        return dataset\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at '{filepath}' was not found.\")\n",
    "    except ImportError:\n",
    "        print(\"Error: Required dependencies for reading NETCDF files are not installed. Please refer to https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# ds = load_netcdf('path/to/netcdf_file.nc')\n",
    "# print(ds)\n",
    "\n",
    "def assemble_datacube(folderpath, crop=(True,(4.4699, 50.5039),6,'summer'), max_files=100):\n",
    "    \"\"\"\n",
    "    Assemble a datacube along the time dimension from multiple NETCDF files in a folder until a certain maximum number of files is reached.\n",
    "    \n",
    "    Parameters:\n",
    "    folderpath (str): Path to the folder containing NETCDF files.\n",
    "    max_files (int, optional): The maximum number of NETCDF files to read for the assembled datacube. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: The assembled datacube.\n",
    "    \"\"\"\n",
    "    \n",
    "    datacube = None  # Initialize an empty datacube to concatenate into\n",
    "    file_count = 0\n",
    "\n",
    "    try:\n",
    "        for filename in os.listdir(folderpath):\n",
    "            if filename.endswith(\".nc\"):\n",
    "                filepath = os.path.join(folderpath, filename)\n",
    "                if crop[0]:\n",
    "                    ds = mask_time(mask_square(load_netcdf(filepath), crop[1], crop[2]),'summer')\n",
    "                else:\n",
    "                    ds = load_netcdf(filepath)\n",
    "                if datacube is None:\n",
    "                    datacube = ds  # Initialize the datacube\n",
    "                else:\n",
    "                    datacube = xr.concat([datacube, ds], dim='time')  # Concatenate along time dimension\n",
    "                file_count += 1\n",
    "                if file_count >= max_files:\n",
    "                    break\n",
    "\n",
    "        if datacube is not None:\n",
    "            datacube['lon'] = convert_360_to_180(datacube['lon'])  # Convert longitudes if needed\n",
    "            datacube = datacube.sortby(['lat', 'lon'])  # Sort by lat and lon\n",
    "            return datacube\n",
    "        else:\n",
    "            print(\"No valid NETCDF files found in the folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while assembling the datacube: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# datacube = assemble_datacube('path/to/folder')\n",
    "# print(datacube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎨 Plotting Section: Static and Dynamic Visualization Functions\n",
    "\n",
    "This section focuses on plotting data either statically or dynamically using animations in **Matplotlib**. The functions in this section use the loaded data from **Section 2** and allow users to visualize the masks applied in **Section 1**, both in time and space. These visualizations are powerful tools to ensure the data is well-conditioned in both dimensions and provide insightful visual analysis.\n",
    "\n",
    "#### 📊 Static Plots:\n",
    "- Visualize data at specific time steps or spatial regions.\n",
    "- Check how masks affect the data over time or space.\n",
    "\n",
    "#### 🎞️ Dynamic Plots (Animations):\n",
    "- Create time-lapse visualizations to observe temporal evolution.\n",
    "- Useful for understanding changes in datasets across multiple time frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def plot_datacube(ds, variable='pr', time_step=0, ax=None, cbar_ax=None, fig=None, figsize=(10, 5), norm=None, interpolation=None, cmap='Blues'):\n",
    "    \"\"\"\n",
    "    Plot a single time step of the datacube for a specified variable, or just the data if no time dimension exists.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset containing the data to be plotted.\n",
    "    variable (str, optional): The name of the variable to plot. Default is 'pr'.\n",
    "    time_step (int, optional): The time step to plot. Default is 0.\n",
    "    ax (matplotlib.axes._subplots.AxesSubplot, optional): Axis to plot on. Default is None.\n",
    "    cbar_ax (matplotlib.axes._subplots.AxesSubplot, optional): Axis for the colorbar. Default is None.\n",
    "    fig (matplotlib.figure.Figure, optional): Figure object for adding the colorbar. Default is None.\n",
    "    figsize (tuple, optional): Figure size. Default is (10, 5).\n",
    "    norm (optional): Normalization for color mapping. Default is None.\n",
    "    interpolation (str, optional): Interpolation method for imshow. Default is None.\n",
    "    cmap (str or Colormap, optional): Colormap for the plot. Default is 'Blues'.\n",
    "    \"\"\"\n",
    "    if ax is None or cbar_ax is None or fig is None:\n",
    "        gs = GridSpec(1, 2, width_ratios=[1, 0.05])\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(gs[0], projection=ccrs.PlateCarree())\n",
    "        cbar_ax = fig.add_subplot(gs[1])\n",
    "\n",
    "    # Check if the variable exists in the dataset\n",
    "    if variable not in ds:\n",
    "        raise ValueError(f\"Variable '{variable}' not found in the dataset.\")\n",
    "\n",
    "    # Check if the dataset has a time dimension and select the data accordingly\n",
    "    if 'time' in ds.dims:\n",
    "        data = ds[variable].isel(time=time_step)\n",
    "        title = f\"{variable.upper()} Plot for Date: {str(ds.time.values[time_step])}\"\n",
    "    else:\n",
    "        data = ds[variable]\n",
    "        title = f\"{variable.upper()} Plot\"\n",
    "\n",
    "    # Plot the data\n",
    "    img = ax.imshow(data, origin='upper', transform=ccrs.PlateCarree(), cmap=cmap,\n",
    "                    extent=[ds['lon'].min(), ds['lon'].max(), ds['lat'].min(), ds['lat'].max()],\n",
    "                    interpolation=interpolation, norm=norm)  # Allow selection of interpolation and colormap\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(ftr.BORDERS, linestyle=':')\n",
    "    gl = ax.gridlines(draw_labels=True, crs=ccrs.PlateCarree(), color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {'size': 10, 'color': 'black'}\n",
    "    gl.ylabel_style = {'size': 10, 'color': 'black'}\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Update colorbar with the variable's name and units (default to 'unknown units' if not provided)\n",
    "    units = ds[variable].attrs.get('units', 'unknown units')\n",
    "    fig.colorbar(img, ax=ax, cax=cbar_ax, shrink=0.5, aspect=5, label=f\"{variable} ({units})\")\n",
    "\n",
    "    if ax is None or cbar_ax is None or fig is None:\n",
    "        plt.show()\n",
    "\n",
    "#global_min, global_max = CMISP_summer['pr'].min(), CMISP_summer['pr'].quantile(0.975)\n",
    "#norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "#plot_datacube(CMISP_summer, figsize = (8,6),norm=norm)\n",
    "def animate_datacube(ds, variable='pr', time_range=10, norm=None, interpolation=None, cmap='Blues'):\n",
    "    \"\"\"\n",
    "    Create an animation of the datacube over the time dimension for a specified variable.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset containing the data to be animated.\n",
    "    variable (str, optional): The name of the variable to animate. Default is 'pr'.\n",
    "    time_range (int, optional): The number of time steps to animate. Default is 10.\n",
    "    norm (optional): Normalization for color mapping. Default is None.\n",
    "    interpolation (str, optional): Interpolation method for imshow. Default is None.\n",
    "    cmap (str or Colormap, optional): Colormap for the animation. Default is 'Blues'.\n",
    "    \"\"\"\n",
    "    # Set up the figure and axis\n",
    "    gs = GridSpec(1, 2, width_ratios=[0.9, 0.05])\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(gs[0], projection=ccrs.PlateCarree())\n",
    "    cbar_ax = fig.add_subplot(gs[1])\n",
    "\n",
    "    # Update function for animation\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        plot_datacube(ds, variable=variable, time_step=frame, ax=ax, cbar_ax=cbar_ax, fig=fig, norm=norm, interpolation=interpolation, cmap=cmap)\n",
    "\n",
    "    # Create the animation\n",
    "    ani = animation.FuncAnimation(fig, update, frames=min(time_range, ds.dims.get('time', 1)), interval=500)\n",
    "    plt.rcParams['animation.embed_limit'] = 50  # Set embed limit to 50 MB\n",
    "    return HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def save_tempanomaly(ds, filepath, blockpath, start_year='1950-01-01', end_year='2024-12-31'):\n",
    "    \"\"\"\n",
    "    Calculate the annual mean temperature anomaly from the given dataset and save it as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset containing the temperature anomaly data ('tempanomaly').\n",
    "    filepath (str): The path to save the resulting CSV file.\n",
    "    start_year (str, optional): The start year for the data to be considered. Default is '1950-01-01'.\n",
    "    end_year (str, optional): The end year for the data to be considered. Default is '2024-12-31'.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with years and mean temperature anomalies.\n",
    "\n",
    "    Example usage:\n",
    "    save_tempanomaly(GISTEMP, 'output/annual_mean_tempanomaly.csv')\n",
    "    \"\"\"\n",
    "    # Filter the dataset to only include years between start_year and end_year\n",
    "    ds_filtered = ds.sel(time=slice(start_year, end_year))\n",
    "    \n",
    "    # Resample the data to an annual frequency and calculate the mean for each year across 'lat' and 'lon'\n",
    "    annual_mean_tempanomaly = ds_filtered['tempanomaly'].resample(time='1Y').mean(dim=['time', 'lat', 'lon']).to_dataframe().reset_index()\n",
    "    \n",
    "    # Extract the year from the time column and create a 'year' column\n",
    "    annual_mean_tempanomaly['year'] = annual_mean_tempanomaly['time'].apply(lambda x: x.year)\n",
    "    annual_mean_tempanomaly = annual_mean_tempanomaly[['year', 'tempanomaly']]\n",
    "    annual_mean_tempanomaly.rename(columns={'tempanomaly': 'tempanomalyMean'}, inplace=True)\n",
    "    \n",
    "    # Check if the Blockmax CSV file exists\n",
    "    blockmax_filepath = blockpath\n",
    "    if not os.path.exists(blockmax_filepath):\n",
    "        raise FileNotFoundError(f\"The file '{blockmax_filepath}' was not found.\")\n",
    "    \n",
    "    # Load the Blockmax DataFrame and merge with the annual mean temperature anomaly DataFrame\n",
    "    blockmax_df = pd.read_csv(blockmax_filepath)\n",
    "    merged_df = pd.merge(blockmax_df, annual_mean_tempanomaly, on='year')\n",
    "    \n",
    "    # Save the resulting DataFrame as a CSV file\n",
    "    merged_df.to_csv(filepath, index=False)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 EVT Analysis Section: Block Maxima and Peak Over Threshold\n",
    "\n",
    "This section focuses on transforming the data to correspond with the two main philosophies of Extreme Value Theory (EVT): **Block Maxima** and **Peak Over Threshold (POT)**. These transformations help prepare the data for EVT analysis, which is crucial for studying extreme events.\n",
    "\n",
    "#### 🌐 EVT Techniques:\n",
    "1. **Block Maxima**: Divides the dataset into blocks (e.g., yearly or monthly) and retains only the maximum value from each block, allowing for the analysis of extreme values within those blocks.\n",
    "2. **Peak Over Threshold (POT)**: Focuses on values that exceed a predefined threshold, retaining all data points that represent extremes beyond that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def block_maximum(ds):\n",
    "    \"\"\"\n",
    "    Calculate the maximum precipitation value per year over all spatial dimensions and time jointly, and return a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The xarray dataset containing the precipitation data ('pr'). The dataset should be masked in space and time.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with years as the index and maximum precipitation values as the column.\n",
    "\n",
    "    Example usage:\n",
    "    df_max_precip = block_maximum(ds_summer)\n",
    "    \"\"\"\n",
    "    # Resample the dataset to annual frequency and take the maximum value for each year over all dimensions (time, lat, lon)\n",
    "    annual_max = ds['pr'].resample(time='1Y').max(dim=['time', 'lat', 'lon'])\n",
    "    # Create a DataFrame from the resampled data\n",
    "    df_max_precip = annual_max.to_dataframe().reset_index()\n",
    "    # Extract only the year from the time column\n",
    "    df_max_precip['year'] = df_max_precip['time'].apply(lambda x: x.year)\n",
    "    df_max_precip = df_max_precip[['year', 'pr']]\n",
    "    df_max_precip.rename(columns={'pr': 'prmax'}, inplace=True)\n",
    "    return df_max_precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💾 Export Section: Export Processed Data to CSV\n",
    "\n",
    "This section handles the export of processed data, such as the results from the EVT analysis, to CSV format for further use or sharing. This ensures that the processed data is stored in a widely compatible format, allowing for easy access and use outside the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def export_csv(df, filepath, overwrite=True):\n",
    "    \"\"\"\n",
    "    Export a pandas DataFrame to a CSV file. Allows overwriting if the file already exists.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to be exported.\n",
    "    filepath (str): The path where the CSV file will be saved.\n",
    "    overwrite (bool, optional): Whether to overwrite the file if it already exists. Default is True.\n",
    "\n",
    "    Example usage:\n",
    "    export_csv(df_max_precip, 'output/max_precipitation.csv')\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if os.path.exists(filepath) and not overwrite:\n",
    "        raise FileExistsError(f\"The file '{filepath}' already exists and overwrite is set to False.\")\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "filepath = r\"c:\\Users\\bobel\\Downloads\\pr_day_EC-Earth3_piControl_r1i1p1f1_gr_22590101-22591231.nc\"\n",
    "#export_csv(CMISP_blockmax,filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Practical Section: Applying the Function Corpus to Process Data\n",
    "In this section, we demonstrate how to use the function corpus to process various datasets from start to finish. This includes loading the data, applying space-time masks, performing EVT analysis, visualizing the data, and exporting the processed results.\n",
    "\n",
    "#### 🔄 Step-by-Step Workflow (example):\n",
    "\n",
    "1. **Load the Data**: We first load the dataset (e.g., a space-time datacube) using the functions from the **Loading Section**.\n",
    "2. **Apply Masks**: Next, we crop the dataset spatially, temporally, or both using the masking functions from **Section 1**.\n",
    "3. **EVT Analysis**: After masking, we transform the data using the **Block Maxima** or **Peak Over Threshold** methods from the **EVT Section**.\n",
    "4. **Visualize the Data**: Before finalizing the analysis, we create both static and dynamic visualizations using the **Plotting Section** functions to ensure the data is properly conditioned.\n",
    "5. **Export the Data**: Finally, we export the processed data to CSV format for external use or storage using the **Export Section** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMISP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "CMISP_summer = assemble_datacube(folderpath=r\"c:\\ThesisData\\CMISP\",max_files=100)\n",
    "global_min = EOBS_summer['pr'].min()\n",
    "global_max = EOBS_summer['pr'].quantile(0.975)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "animate_datacube(EOBS_summer.sel(time=slice(EOBS_summer.time[-60], EOBS_summer.time[-1])), time_range=3,norm=norm)\n",
    "CMISP_blockmax = block_maximum(CMISP_summer)\n",
    "CMISP_blockmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOBS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "center_coordinates = (7.0, 50.5)  # Center around Ahr-Eifel basin (longitude, latitude)\n",
    "side_length = 1.5  # Side length of the square in degrees\n",
    "EOBS_summer = mask_time(\n",
    "    mask_square(\n",
    "        load_netcdf(filepath=r\"c:\\ThesisData\\E_OBS\\rr_ens_mean_0.1deg_reg_v30.0e.nc\", coords=('rr', 'latitude', 'longitude')),\n",
    "        center_coordinates,\n",
    "        side_length\n",
    "    ),\n",
    "    season='summer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GISTEMP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "GISTEMP = xr.open_dataset(r\"c:\\Users\\bobel\\Downloads\\gistemp1200_GHCNv4_ERSSTv5.nc\", engine='netcdf4')\n",
    "GISTEMP\n",
    "save_tempanomaly(GISTEMP,r\"c:\\Users\\bobel\\Downloads\\UCL_blockmax.csv\",r\"c:\\Users\\bobel\\Downloads\\UCL_blockmax.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uccles Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"c:\\Users\\bobel\\OneDrive - Université de Namur\\Data\\UCCLES\\Precip_Uccle_1892.csv\")\n",
    "df['DAY'] = pd.to_datetime(df['DAY'])\n",
    "df['year'] = df['DAY'].dt.year\n",
    "# Group by year and calculate the max precipitation for each year\n",
    "max_precip_per_year = df.groupby('year')['PRECIP_QUANTITY'].max().reset_index()\n",
    "max_precip_per_year\n",
    "export_csv(filepath=r\"c:\\Users\\bobel\\OneDrive - Université de Namur\\Data\\Blockmax\\UCL_blockmax.csv\",df=max_precip_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Stationary**: No covariate dependence.\n",
    "   - $ \\mu $, $ \\sigma $, $ \\xi $ are constant.\n",
    "\n",
    "2. **Loc-linear**: Linear dependence on location.\n",
    "   - $ \\mu = \\beta_0 + \\beta_1 x_1 $, $ \\sigma $, $ \\xi $\n",
    "\n",
    "3. **Loc+Shape-linear**: Linear dependence on location and shape.\n",
    "   - $ \\mu = \\beta_0 + \\beta_1 x_1 $, $ \\sigma = \\alpha_0 + \\alpha_0 x_1 $, $ \\xi $\n",
    "\n",
    "4. **Loc+Scale+Shape-linear**: Linear dependence on location, scale, and shape.\n",
    "   - $ \\mu = \\beta_0 + \\beta_1 x_1 $, $ \\sigma = \\alpha_0 + \\alpha_1 x_1 $, $ \\xi = \\gamma_0 + \\gamma_1 x_1 $\n",
    "\n",
    "5. **Stationary (scale exp link)**: Stationary model with an exponential link on shape.\n",
    "   - $ \\mu $, $ e^{\\alpha_0} $, $ \\xi $\n",
    "\n",
    "6. **Loc-linear (scale exp link)**: Linear dependence on location with an exponential link on shape.\n",
    "   - $ \\mu = \\beta_0 + \\beta_1 x_1 $, $\\sigma = e^{\\alpha_0}$, $ \\xi $\n",
    "\n",
    "7. **Loc+Shape-linear (scale exp link)**: Linear dependence on location and shape, with an exponential link on shape.\n",
    "   - $ \\mu = \\beta_0 + \\beta_1 x_1 $, $ \\sigma = e^{\\alpha_0 + \\alpha_1 x_1} $, $ \\xi$\n",
    "\n",
    "8. **Loc+Scale+Shape-linear (scale exp link)**: Linear dependence on location, scale, and shape, with an exponential link on shape.\n",
    "   - $ \\mu = \\beta_0 + \\beta_1 x_1 $, $ \\sigma = e^{\\alpha_0 + \\alpha_1 x_1} $, $ \\xi = \\gamma_0 + \\gamma_1 x_1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "EOBS = pd.read_csv(r\"c:\\ThesisData\\OUTPUTS\\UCL_blockmax.csv\")\n",
    "exog1 = {}\n",
    "exog2 = {'location':EOBS['tempanomalyMean']}\n",
    "exog3 = {'location':EOBS['tempanomalyMean'],\n",
    "'scale' : EOBS['tempanomalyMean']}\n",
    "exog4 = {'location':EOBS['tempanomalyMean'],\n",
    "'shape' : EOBS['tempanomalyMean'], 'scale': EOBS['tempanomalyMean']}\n",
    "exog5 = {'location':EOBS['tempanomalyMean'].values,\n",
    "'scale' : EOBS['tempanomalyMean'].values}\n",
    "fit1 = GEV(endog=EOBS[\"prmax\"],exog=exog1,full_output=True).fit()\n",
    "fit2 = GEV(endog=EOBS[\"prmax\"],exog=exog2,full_output=True).fit()\n",
    "fit3 = GEV(endog=EOBS[\"prmax\"],exog=exog3,full_output=True).fit()\n",
    "fit4 = GEV(endog=EOBS[\"prmax\"],exog=exog4,full_output=True).fit()\n",
    "fit5 = GEV(endog=EOBS[\"prmax\"],exog=exog,scale_link=EVTModel.exp_link,full_output=True).fit()\n",
    "fit6 = GEV(endog=EOBS[\"prmax\"],exog=exog2,scale_link=EVTModel.exp_link, full_output=True).fit()\n",
    "fit7 = GEV(endog=EOBS[\"prmax\"],exog=exog3,scale_link=EVTModel.exp_link, full_output=True).fit()\n",
    "fit8 = GEV(endog=EOBS[\"prmax\"],exog=exog4,scale_link=EVTModel.exp_link, full_output=True).fit()\n",
    "fit9 = GEVTradowsky(endog=EOBS[\"prmax\"].values,exog=exog5, full_output=True).fit()\n",
    "\n",
    "import pandas as pd\n",
    "# Assuming each fit object has .AIC() and .BIC() methods\n",
    "# Replace these calls with the actual AIC and BIC values from your model fits\n",
    "data = {\n",
    "    \"Model Name\": [\n",
    "        \"Stationary\",\n",
    "        \"Loc-linear\",\n",
    "        \"Loc+scale-linear\",\n",
    "        \"Loc+Scale+Shape-linear\",\n",
    "        \"Stationary\",\n",
    "        \"Loc-linear\",\n",
    "        \"Loc+exp(Scale)-linear\",\n",
    "        \"Loc+exp(Scale)+Shape-linear\",\n",
    "        \"Tradowsky model\"\n",
    "    ],\n",
    "    \"AIC\": [fit1.AIC(), fit2.AIC(), fit3.AIC(), fit4.AIC(), fit5.AIC(), fit6.AIC(), fit7.AIC(), fit8.AIC(),fit9.AIC()],\n",
    "    \"BIC\": [fit1.BIC(), fit2.BIC(), fit3.BIC(), fit4.BIC(), fit5.BIC(), fit6.BIC(), fit7.BIC(), fit8.BIC(),fit9.BIC()]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Use pandas Styler to format the table nicely\n",
    "styled_df = df.style.set_table_styles([\n",
    "    {'selector': 'thead th', 'props': [('background-color', '#4CAF50'), ('color', 'white'), ('font-weight', 'bold')]},\n",
    "    {'selector': 'tbody td', 'props': [('border', '1px solid #ddd'), ('padding', '8px')]}\n",
    "]).format(precision=2).set_properties(**{\n",
    "    'text-align': 'center',\n",
    "    'font-family': 'Arial, sans-serif',\n",
    "    'border-collapse': 'collapse'\n",
    "}).set_caption(\"Model Comparison: AIC and BIC Scores\")\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "print(\"LINEAR LOCATION MODEL\")\n",
    "print(fit1)\n",
    "print(\"TRADOWSKY MODEL\")\n",
    "print(fit9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters map (location, scale, shape, alpha {Tradowsky})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "center_coordinates = (7.0, 50.5)  # Center around Ahr-Eifel basin (longitude, latitude)\n",
    "side_length = 2  # Side length of the square in degrees\n",
    "EOBS_summer = mask_square(mask_time(load_netcdf(filepath=r\"c:\\ThesisData\\E_OBS\\rr_ens_mean_0.1deg_reg_v30.0e.nc\", coords=('rr', 'latitude', 'longitude')),\n",
    "'summer'),center_coordinates,side_length)\n",
    "EOBS_summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def block_maximum_grid(ds): \n",
    "    \"\"\"\n",
    "    Calculate the maximum precipitation value per year for each pixel individually and return an xarray Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The xarray dataset containing the precipitation data ('pr').\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A Dataset with yearly maximum precipitation values for each pixel.\n",
    "    \"\"\"\n",
    "    # Resample the dataset to annual frequency and take the maximum value for each year over the time dimension only\n",
    "    annual_max = ds['pr'].resample(time='1Y').max(dim='time')\n",
    "    annual_max_ds = annual_max.to_dataset(name='pr')\n",
    "    \n",
    "    # The result is an xarray Dataset with the maximum precipitation per year for each lat-lon pixel\n",
    "    return annual_max_ds\n",
    "\n",
    "EOBS_blockmax_grid = block_maximum_grid(EOBS_summer)\n",
    "global_min = EOBS_blockmax_grid['pr'].min()\n",
    "global_max = EOBS_blockmax_grid['pr'].quantile(0.975)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def fit_gev_model(ds, exog={}):\n",
    "    \"\"\"\n",
    "    Fit a GEV model to each spatial pixel's time series of prmax values and store the results in an xarray Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset containing the annual maximum precipitation ('prmax') for each pixel.\n",
    "    exog (array-like): The exogenous variables used in the GEV model.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A Dataset containing the fitted parameters for each spatial pixel.\n",
    "    \"\"\"\n",
    "    # Initialize arrays to store the model parameters for each pixel\n",
    "    shape_param = np.full((ds.dims['lat'], ds.dims['lon']), np.nan)\n",
    "    location_param = np.full((ds.dims['lat'], ds.dims['lon']), np.nan)\n",
    "    scale_param = np.full((ds.dims['lat'], ds.dims['lon']), np.nan)\n",
    "    \n",
    "    # Loop over each spatial pixel\n",
    "    for i, lat in enumerate(ds['lat'].values):\n",
    "        for j, lon in enumerate(ds['lon'].values):\n",
    "            # Extract the time series for the current pixel\n",
    "            prmax_series = ds['pr'].sel(lat=lat, lon=lon).values\n",
    "            \n",
    "            # Check if the time series is valid (e.g., not all NaNs)\n",
    "            if np.any(~np.isnan(prmax_series)):\n",
    "                # Fit the GEV model\n",
    "                model = GEV(endog=prmax_series, exog=exog, full_output=True)\n",
    "                fit_result = model.fit()\n",
    "                \n",
    "                # Store the fitted parameters\n",
    "                location_param[i, j] = fit_result.like.x[0]\n",
    "                shape_param[i, j] = fit_result.like.x[1]\n",
    "                scale_param[i, j] = fit_result.like.x[2]\n",
    "\n",
    "    # Create a Dataset to store the fitted parameters\n",
    "    fitted_params_ds = xr.Dataset(\n",
    "        {\n",
    "            'shape': (['lat', 'lon'], shape_param),\n",
    "            'location': (['lat', 'lon'], location_param),\n",
    "            'scale': (['lat', 'lon'], scale_param)\n",
    "        },\n",
    "        coords={\n",
    "            'lat': ds['lat'],\n",
    "            'lon': ds['lon']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return fitted_params_ds\n",
    "\n",
    "EOBS_gev_grid = fit_gev_model(EOBS_blockmax_grid)\n",
    "EOBS_gev_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "global_min = EOBS_gev_grid['shape'].quantile(0.005)\n",
    "global_max = EOBS_gev_grid['shape'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_grid,variable='shape',cmap='coolwarm',norm=norm,interpolation='bilinear')\n",
    "global_min = EOBS_gev_grid['location'].min()\n",
    "global_max = EOBS_gev_grid['location'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_grid,variable='location',cmap='coolwarm',norm=norm,interpolation='bilinear')\n",
    "global_min = -0.3\n",
    "global_max = EOBS_gev_grid['scale'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_grid,variable='scale',cmap='coolwarm',norm=norm,interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df = EOBS_gev_grid.to_dataframe().reset_index()\n",
    "\n",
    "# Plot pairwise scatter plots for 'shape', 'location', and 'scale'\n",
    "sns.pairplot(df, vars=['shape', 'location', 'scale'], kind='scatter')\n",
    "plt.suptitle(\"Scatter Plot Matrix of Shape, Location, and Scale Parameters\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "center_coordinates = (7.0, 50.5)  # Center around Ahr-Eifel basin (longitude, latitude)\n",
    "side_length = 2  # Side length of the square in degrees\n",
    "EOBS_gev_large_w = fit_gev_model(block_maximum_grid(mask_time(\n",
    "    mask_square(\n",
    "        load_netcdf(filepath=r\"c:\\ThesisData\\E_OBS\\rr_ens_mean_0.1deg_reg_v30.0e.nc\", coords=('rr', 'latitude', 'longitude')),\n",
    "        center_coordinates,\n",
    "        side_length\n",
    "    ),\n",
    "    season='winter'\n",
    ")))\n",
    "global_min = EOBS_gev_large_w['shape'].quantile(0.005)\n",
    "global_max = EOBS_gev_large_w['shape'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_large_w,variable='shape',cmap='coolwarm',norm=norm,interpolation='bilinear')\n",
    "global_min = EOBS_gev_large_w['location'].min()\n",
    "global_max = EOBS_gev_large_w['location'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_large_w,variable='location',cmap='coolwarm',norm=norm,interpolation='bilinear')\n",
    "global_min = -0.3\n",
    "global_max = EOBS_gev_large_w['scale'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_large_w,variable='scale',cmap='coolwarm',norm=norm,interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "center_coordinates = (7.0, 50.5)  # Center around Ahr-Eifel basin (longitude, latitude)\n",
    "side_length = 15  # Side length of the square in degrees\n",
    "EOBS_gev_large = fit_gev_model(block_maximum_grid(mask_time(\n",
    "    mask_square(\n",
    "        load_netcdf(filepath=r\"c:\\ThesisData\\E_OBS\\rr_ens_mean_0.1deg_reg_v30.0e.nc\", coords=('rr', 'latitude', 'longitude')),\n",
    "        center_coordinates,\n",
    "        side_length\n",
    "    ),\n",
    "    season='summer'\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "global_min = EOBS_gev_large['shape'].min()\n",
    "global_max = EOBS_gev_large['shape'].quantile(0.95)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_large,variable='shape',cmap='coolwarm',norm=norm)\n",
    "global_min = EOBS_gev_large['location'].min()\n",
    "global_max = EOBS_gev_large['location'].quantile(0.95)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_large,variable='location',cmap='coolwarm',norm=norm)\n",
    "global_min = -0.2\n",
    "global_max = EOBS_gev_large['scale'].quantile(0.975)\n",
    "norm = colors.TwoSlopeNorm(vmin=global_min, vcenter=0, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_large,variable='scale',cmap='coolwarm',norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "EOBS = pd.read_csv(r\"c:\\ThesisData\\OUTPUTS\\UCL_blockmax.csv\")\n",
    "def fit_gevTradowsky_model(ds, exog={'location':EOBS['tempanomalyMean'].values, 'scale':EOBS['tempanomalyMean'].values}):\n",
    "    \"\"\"\n",
    "    Fit a GEV model to each spatial pixel's time series of prmax values and store the results in an xarray Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset containing the annual maximum precipitation ('prmax') for each pixel.\n",
    "    exog (array-like): The exogenous variables used in the GEV model.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A Dataset containing the fitted parameters for each spatial pixel.\n",
    "    \"\"\"\n",
    "    # Initialize arrays to store the model parameters for each pixel\n",
    "    alpha_param = np.full((ds.dims['lat'], ds.dims['lon']), np.nan)\n",
    "    shape_param = np.full((ds.dims['lat'], ds.dims['lon']), np.nan)\n",
    "    \n",
    "    # Loop over each spatial pixel\n",
    "    for i, lat in enumerate(ds['lat'].values):\n",
    "        for j, lon in enumerate(ds['lon'].values):\n",
    "            # Extract the time series for the current pixel\n",
    "            prmax_series = ds['pr'].sel(lat=lat, lon=lon).values\n",
    "            \n",
    "            # Check if the time series is valid (e.g., not all NaNs)\n",
    "            if np.any(~np.isnan(prmax_series)):\n",
    "                # Fit the GEV model\n",
    "                model = GEVTradowsky(endog=prmax_series, exog=exog, full_output=True)\n",
    "                fit_result = model.fit()\n",
    "                \n",
    "                # Store the fitted parameters\n",
    "                alpha_param[i, j] = fit_result.like.x[0]\n",
    "                shape_param[i, j] = fit_result.like.x[1]\n",
    "\n",
    "\n",
    "    # Create a Dataset to store the fitted parameters\n",
    "    fitted_params_ds = xr.Dataset(\n",
    "        {\n",
    "            'alpha': (['lat', 'lon'], alpha_param),\n",
    "            'shape': (['lat', 'lon'], shape_param),\n",
    "        },\n",
    "        coords={\n",
    "            'lat': ds['lat'],\n",
    "            'lon': ds['lon']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return fitted_params_ds\n",
    "\n",
    "EOBS_gev_grid_tr = fit_gevTradowsky_model(EOBS_blockmax_grid)\n",
    "EOBS_gev_grid_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "global_min = EOBS_gev_grid_tr['alpha'].quantile(0.005)\n",
    "global_max = EOBS_gev_grid_tr['alpha'].quantile(0.995)\n",
    "norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_grid_tr,variable='alpha',cmap='coolwarm',norm=norm,interpolation='bilinear')\n",
    "global_min = EOBS_gev_grid_tr['shape'].quantile(0.005)\n",
    "global_max = EOBS_gev_grid_tr['shape'].quantile(0.995)\n",
    "norm = colors.TwoSlopeNorm(vmin=global_min, vcenter=0, vmax=global_max)\n",
    "plot_datacube(EOBS_gev_grid_tr,variable='shape',cmap='coolwarm',norm=norm,interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "phd-python"
    },
    "polyglot_notebook": {
     "kernelName": "phd-python"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df = EOBS_gev_grid_tr.to_dataframe().reset_index()\n",
    "\n",
    "sns.pairplot(df, vars=['alpha', 'shape'], kind='scatter')\n",
    "plt.suptitle(\"Scatter Plot Matrix of Aplha shape Parameters\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📘 R : Library Import and Global Variable Definition\n",
    "\n",
    "In this section, I import the necessary libraries 📦 for data manipulation and analysis in R. Additionally, I define global variables 🌐 (mostly path-related), which can be reused throughout the code for various locations or users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We switch To R to fit the GEV now that we have our dataset(s) ready and built. We will use ismev for estimation and ggplot for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "\n",
      "Loading required package: mgcv\n",
      "\n",
      "Loading required package: nlme\n",
      "\n",
      "\n",
      "Attaching package: 'nlme'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    collapse\n",
      "\n",
      "\n",
      "This is mgcv 1.8-28. For overview type 'help(\"mgcv-package\")'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(ismev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "EOBS <- read.csv(\"c:\\\\ThesisData\\\\OUTPUTS\\\\UCL_blockmax.csv\")\n",
    "#UCL <- read.csv(\"c:\\\\Users\\\\bobel\\\\OneDrive - Université de Namur\\\\Data\\\\Blockmax\\\\UCL_blockmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(EOBS, aes(x = tempanomalyMean, y = prmax)) +\n",
    "  geom_point(color = 'blue', size = 3, alpha = 0.7) +\n",
    "  labs(\n",
    "    title = \"Maximum Precipitation vs Temperature Anomaly\",\n",
    "    x = \"Temperature Anomaly (Mean)\",\n",
    "    y = \"Maximum Precipitation (prmax)\"\n",
    "  ) +\n",
    "  theme_minimal() +  # A simple, clean theme for the plot\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n",
    "    axis.title = element_text(size = 14),\n",
    "    axis.text = element_text(size = 12),\n",
    "    panel.grid.major = element_line(size = 0.5, linetype = \"dotted\", color = \"gray\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$conv\n",
      "[1] 0\n",
      "\n",
      "$nllh\n",
      "[1] 307.7596\n",
      "\n",
      "$mle\n",
      "[1] 55.8991204 12.3197885  0.0250356\n",
      "\n",
      "$se\n",
      "[1] 1.60016258 1.16794858 0.08452555\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$trans\n",
       "[1] FALSE\n",
       "\n",
       "$model\n",
       "$model[[1]]\n",
       "NULL\n",
       "\n",
       "$model[[2]]\n",
       "NULL\n",
       "\n",
       "$model[[3]]\n",
       "NULL\n",
       "\n",
       "\n",
       "$link\n",
       "[1] \"c(identity, identity, identity)\"\n",
       "\n",
       "$conv\n",
       "[1] 0\n",
       "\n",
       "$nllh\n",
       "[1] 307.7596\n",
       "\n",
       "$data\n",
       " [1]  39.20000  45.60000  65.00000  58.50000  83.10000  57.50000  69.50000\n",
       " [8]  45.30000  54.50000  70.70001  59.60000  70.50000  39.10000  56.90000\n",
       "[15]  60.70000  87.30000  82.10000  63.50000  84.90000  54.20000  76.70001\n",
       "[22]  39.20000  53.30000  43.40000  54.50000  59.10000  55.10000  40.80000\n",
       "[29]  96.10000  52.00000  58.50000  98.60000  50.30000  58.10000  58.20000\n",
       "[36]  52.30000  48.10000  57.90000  61.50000  39.10000  46.10000  47.80000\n",
       "[43]  67.00000  54.40000  65.10000  70.70001  74.10000  48.10000  73.80000\n",
       "[50]  64.90000  67.00000  52.50000  71.70001  53.70000  51.30000  58.30000\n",
       "[57]  88.00000  78.00000  70.10000  60.60000 119.80000  48.80000  60.90000\n",
       "[64]  65.80000  58.30000  57.50000  76.90000  76.80000  59.30000  78.50000\n",
       "[71]  52.90000 116.90000  55.60000  89.10000  68.80000\n",
       "\n",
       "$mle\n",
       "[1] 55.8991204 12.3197885  0.0250356\n",
       "\n",
       "$cov\n",
       "            [,1]        [,2]         [,3]\n",
       "[1,]  2.56052027  0.75865826 -0.046528154\n",
       "[2,]  0.75865826  1.36410388 -0.024793836\n",
       "[3,] -0.04652815 -0.02479384  0.007144569\n",
       "\n",
       "$se\n",
       "[1] 1.60016258 1.16794858 0.08452555\n",
       "\n",
       "$vals\n",
       "          [,1]     [,2]      [,3]\n",
       " [1,] 55.89912 12.31979 0.0250356\n",
       " [2,] 55.89912 12.31979 0.0250356\n",
       " [3,] 55.89912 12.31979 0.0250356\n",
       " [4,] 55.89912 12.31979 0.0250356\n",
       " [5,] 55.89912 12.31979 0.0250356\n",
       " [6,] 55.89912 12.31979 0.0250356\n",
       " [7,] 55.89912 12.31979 0.0250356\n",
       " [8,] 55.89912 12.31979 0.0250356\n",
       " [9,] 55.89912 12.31979 0.0250356\n",
       "[10,] 55.89912 12.31979 0.0250356\n",
       "[11,] 55.89912 12.31979 0.0250356\n",
       "[12,] 55.89912 12.31979 0.0250356\n",
       "[13,] 55.89912 12.31979 0.0250356\n",
       "[14,] 55.89912 12.31979 0.0250356\n",
       "[15,] 55.89912 12.31979 0.0250356\n",
       "[16,] 55.89912 12.31979 0.0250356\n",
       "[17,] 55.89912 12.31979 0.0250356\n",
       "[18,] 55.89912 12.31979 0.0250356\n",
       "[19,] 55.89912 12.31979 0.0250356\n",
       "[20,] 55.89912 12.31979 0.0250356\n",
       "[21,] 55.89912 12.31979 0.0250356\n",
       "[22,] 55.89912 12.31979 0.0250356\n",
       "[23,] 55.89912 12.31979 0.0250356\n",
       "[24,] 55.89912 12.31979 0.0250356\n",
       "[25,] 55.89912 12.31979 0.0250356\n",
       "[26,] 55.89912 12.31979 0.0250356\n",
       "[27,] 55.89912 12.31979 0.0250356\n",
       "[28,] 55.89912 12.31979 0.0250356\n",
       "[29,] 55.89912 12.31979 0.0250356\n",
       "[30,] 55.89912 12.31979 0.0250356\n",
       "[31,] 55.89912 12.31979 0.0250356\n",
       "[32,] 55.89912 12.31979 0.0250356\n",
       "[33,] 55.89912 12.31979 0.0250356\n",
       "[34,] 55.89912 12.31979 0.0250356\n",
       "[35,] 55.89912 12.31979 0.0250356\n",
       "[36,] 55.89912 12.31979 0.0250356\n",
       "[37,] 55.89912 12.31979 0.0250356\n",
       "[38,] 55.89912 12.31979 0.0250356\n",
       "[39,] 55.89912 12.31979 0.0250356\n",
       "[40,] 55.89912 12.31979 0.0250356\n",
       "[41,] 55.89912 12.31979 0.0250356\n",
       "[42,] 55.89912 12.31979 0.0250356\n",
       "[43,] 55.89912 12.31979 0.0250356\n",
       "[44,] 55.89912 12.31979 0.0250356\n",
       "[45,] 55.89912 12.31979 0.0250356\n",
       "[46,] 55.89912 12.31979 0.0250356\n",
       "[47,] 55.89912 12.31979 0.0250356\n",
       "[48,] 55.89912 12.31979 0.0250356\n",
       "[49,] 55.89912 12.31979 0.0250356\n",
       "[50,] 55.89912 12.31979 0.0250356\n",
       "[51,] 55.89912 12.31979 0.0250356\n",
       "[52,] 55.89912 12.31979 0.0250356\n",
       "[53,] 55.89912 12.31979 0.0250356\n",
       "[54,] 55.89912 12.31979 0.0250356\n",
       "[55,] 55.89912 12.31979 0.0250356\n",
       "[56,] 55.89912 12.31979 0.0250356\n",
       "[57,] 55.89912 12.31979 0.0250356\n",
       "[58,] 55.89912 12.31979 0.0250356\n",
       "[59,] 55.89912 12.31979 0.0250356\n",
       "[60,] 55.89912 12.31979 0.0250356\n",
       "[61,] 55.89912 12.31979 0.0250356\n",
       "[62,] 55.89912 12.31979 0.0250356\n",
       "[63,] 55.89912 12.31979 0.0250356\n",
       "[64,] 55.89912 12.31979 0.0250356\n",
       "[65,] 55.89912 12.31979 0.0250356\n",
       "[66,] 55.89912 12.31979 0.0250356\n",
       "[67,] 55.89912 12.31979 0.0250356\n",
       "[68,] 55.89912 12.31979 0.0250356\n",
       "[69,] 55.89912 12.31979 0.0250356\n",
       "[70,] 55.89912 12.31979 0.0250356\n",
       "[71,] 55.89912 12.31979 0.0250356\n",
       "[72,] 55.89912 12.31979 0.0250356\n",
       "[73,] 55.89912 12.31979 0.0250356\n",
       "[74,] 55.89912 12.31979 0.0250356\n",
       "[75,] 55.89912 12.31979 0.0250356\n",
       "\n",
       "attr(,\"class\")\n",
       "[1] \"gev.fit\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ydat <- as.matrix(EOBS$tempanomalyMean) \n",
    "# Model 1 : Stationary GEV\n",
    "fit_stationary = gev.fit(EOBS$prmax)\n",
    "xlow <- 100 # Example lower bound\n",
    "xup <- 300   # Example upper bound\n",
    "\n",
    "fit_stationary\n",
    "#gev.prof(z=fit_stationary, m = 1000,  xlow = xlow,xup = xup)\n",
    "#mod test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "xdat <- EOBS$prmax\n",
    "ydat <- as.matrix(EOBS$tempanomalyMean)  # Convert the covariate to a matrix format\n",
    "xdat_positive <- xdat/100\n",
    "\n",
    "fit_trend <- gev.fit(\n",
    "  xdat = xdat_positive,\n",
    "  ydat = ydat,\n",
    "  mul = 1,\n",
    "  sigl = 1,\n",
    "  maxit = 100000,\n",
    "  siglink = exp,\n",
    "  method = \"L-BFGS-B\"    # No covariate for the shape parameter\n",
    ")\n",
    "\n",
    "gev.diag(fit_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "gev_statistical_summary_dynamic <- function(z=gev_fit_result,scale=NULL) {\n",
    "  # Extracting elements from the GEV fit result\n",
    "  nllh <- z$nllh\n",
    "  mle <- z$mle\n",
    "  se <- z$se\n",
    "  conv <- z$conv\n",
    "  n <- length(z$data)  # Number of observations\n",
    "  k <- length(mle)  # Number of parameters\n",
    "  \n",
    "  # AIC and BIC calculation\n",
    "  aic <- 2 * k + 2 * nllh\n",
    "  bic <- log(n) * k + 2 * nllh\n",
    "  \n",
    "  # t-values (mle / se)\n",
    "  t_values <- mle / se\n",
    "  \n",
    "  # p-values using normal approximation for large sample sizes\n",
    "  p_values <- 2 * pnorm(-abs(t_values))  # Two-sided test\n",
    "  \n",
    "  # Create a significance level code\n",
    "  significance_codes <- cut(p_values, \n",
    "                            breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf), \n",
    "                            labels = c(\"***\", \"**\", \"*\", \".\", \" \"), \n",
    "                            right = FALSE)\n",
    "  \n",
    "  # Print the header\n",
    "  if (is.null(scale)) {\n",
    "    cat(\"GEV Fit Results\\n\")\n",
    "  } else {\n",
    "    c = paste(\"GEV Fit Results (Scale: \", scale, \")\\n\",sep = \"\")\n",
    "    cat(c)\n",
    "  }\n",
    "  cat(\"=====================================\\n\")\n",
    "  \n",
    "  # Print negative log-likelihood\n",
    "  cat(\"Negative Log-Likelihood:\", round(nllh, 4), \"\\n\")\n",
    "  \n",
    "  # Print AIC and BIC\n",
    "  cat(\"AIC:\", round(aic, 4), \"\\n\")\n",
    "  cat(\"BIC:\", round(bic, 4), \"\\n\")\n",
    "  \n",
    "  # Coefficient summary table\n",
    "  cat(\"\\nCoefficients:\\n\")\n",
    "  \n",
    "  # Dynamically label parameters\n",
    "  param_names <- c(\"Shape\")  # First parameter is always the shape\n",
    "  \n",
    "  # Add placeholders for additional parameters\n",
    "  if (length(mle) > 1) {\n",
    "    # If there are more than just the shape parameter, label the rest generically\n",
    "    for (i in 2:length(mle)) {\n",
    "      param_names <- c(param_names, paste0(\"par\", i - 1))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Build a dataframe for results\n",
    "  results <- data.frame(\n",
    "    `Estimate` = mle,\n",
    "    `StdError` = se,\n",
    "    \"t|Value\" = t_values,\n",
    "    `Prob.t` = p_values,\n",
    "    `Codes` = significance_codes\n",
    "  )\n",
    "\n",
    "  return <- list(\n",
    "    Model = deparse(substitute(z)),\n",
    "    AIC = aic,\n",
    "    BIC = bic,\n",
    "    ParamQty = k,\n",
    "    Scale = scale\n",
    "  )\n",
    "  rownames(results) <- param_names\n",
    "  \n",
    "  print(results, row.names = TRUE)\n",
    "  \n",
    "  # Convergence status\n",
    "  cat(\"\\nConvergence status (0 means successful):\", conv, \"\\n\")\n",
    "  \n",
    "  # Significance codes explanation\n",
    "  cat(\"---\\n\")\n",
    "  cat(\"Codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\\n\")\n",
    "  \n",
    "  # Return the result invisibly for further use\n",
    "  return\n",
    "}\n",
    "\n",
    "# Usage Example:\n",
    "# Assuming `fit` is the result from calling gev.fit() on your data:\n",
    "gev_statistical_summary_dynamic(z=fit_stationary,scale=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "Rkernel"
    },
    "polyglot_notebook": {
     "kernelName": "Rkernel"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "xdat <- EOBS$prmax\n",
    "ydat <- as.matrix(EOBS$tempanomalyMean)  # Convert the covariate to a matrix format\n",
    "\n",
    "fit_tra = gev.fit(xdat_positive, \n",
    "        ydat = ydat,  # Covariate matrix with T_prime (e.g., temperature)\n",
    "        mul = 1,                 # Use the first column of ydat for mu (location)\n",
    "        sigl = 1,                # Use the first column of ydat for sigma (scale)          # Shape parameter is constant (xi)          # Log link for mu\n",
    "        mulink = exp  ,       # Log link for sigma   # Identity link for xi (constant shape)\n",
    "        maxit = 10000)\n",
    "\n",
    "gev.diag(fit_tra)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     },
     {
      "aliases": [],
      "languageName": "python",
      "name": "phd-python"
     },
     {
      "aliases": [],
      "languageName": "R",
      "name": "Rkernel"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
